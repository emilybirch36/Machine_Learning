{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing \n",
    "\n",
    "Data from: https://archive.ics.uci.edu/ml/datasets/Communities+and+Crime\n",
    "\n",
    "1. Data Cleaning\n",
    "- Dealing with data types e.g. making categorical data numeric i.e. dummy features\n",
    "- Handling missing data (Imputing Na values instead of removing)\n",
    "\n",
    "2. Data Exploration\n",
    "- Detecting outliers (Tukey IQH or Kernal Density Estimation)\n",
    "- Plotting Distributions. Log transformation of data that is skewed (very long tails) can impove accuracy.\n",
    "- Balance dataset-to prevent the tree from being biased toward the classes that are dominant. Create an equal number of samples from each class by normalising the sum of the sample weights for each class to the same value.\n",
    "\n",
    "3. Feature Enigineering\n",
    "- Interactions between features\n",
    "- Increasing dimensionality vs decreasing dimensionality\n",
    "- Smote? (generates values for the under sampled classes)\n",
    "\n",
    "4. Feature Selection. Discard the least important variables to reduce noise. Good variables are often constructed using ratios, differences, averages of variables etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "\n",
    "# Read data and assign NA to missing values \n",
    "data = pd.read_csv('http://archive.ics.uci.edu/ml/machine-learning-databases/00211/CommViolPredUnnormalizedData.txt',\n",
    "                   sep='\\s*,\\s*',encoding='latin-1',engine='python', na_values=[\"?\"])\n",
    "\n",
    "\n",
    "data.columns = ['communityname','state','countyCode','communityCode','fold','population','householdsize','racepctblack',\n",
    "           'racePctWhite','racePctAsian','racePctHisp','agePct12t21','agePct12t29','agePct16t24','agePct65up',\n",
    "           'numbUrban','pctUrban','medIncome','pctWWage','pctWFarmSelf','pctWInvInc','pctWSocSec','pctWPubAsst',\n",
    "           'pctWRetire','medFamInc','perCapInc','whitePerCap','blackPerCap','indianPerCap','AsianPerCap','OtherPerCap',\n",
    "           'HispPerCap','NumUnderPov','PctPopUnderPov','PctLess9thGrade','PctNotHSGrad','PctBSorMore','PctUnemployed',\n",
    "           'PctEmploy','PctEmplManu','PctEmplProfServ','PctOccupManu','PctOccupMgmtProf','MalePctDivorce',\n",
    "           'MalePctNevMarr','FemalePctDiv','TotalPctDiv','PersPerFam','PctFam2Par','PctKids2Par','PctYoungKids2Par',\n",
    "           'PctTeen2Par','PctWorkMomYoungKids','PctWorkMom','NumKidsBornNeverMar','PctKidsBornNeverMar','NumImmig',\n",
    "           'PctImmigRecent','PctImmigRec5','PctImmigRec8','PctImmigRec10','PctRecentImmig','PctRecImmig5',\n",
    "           'PctRecImmig8','PctRecImmig10','PctSpeakEnglOnly','PctNotSpeakEnglWell','PctLargHouseFam',\n",
    "           'PctLargHouseOccup','PersPerOccupHous','PersPerOwnOccHous','PersPerRentOccHous','PctPersOwnOccup',\n",
    "           'PctPersDenseHous','PctHousLess3BR','MedNumBR','HousVacant','PctHousOccup','PctHousOwnOcc','PctVacantBoarded',\n",
    "           'PctVacMore6Mos','MedYrHousBuilt','PctHousNoPhone','PctWOFullPlumb','OwnOccLowQuart','OwnOccMedVal',\n",
    "           'OwnOccHiQuart','OwnOccQrange','RentLowQ','RentMedian','RentHighQ','RentQrange','MedRent','MedRentPctHousInc',\n",
    "           'MedOwnCostPctInc','MedOwnCostPctIncNoMtg','NumInShelters','NumStreet','PctForeignBorn','PctBornSameState',\n",
    "           'PctSameHouse85','PctSameCity85','PctSameState85','LemasSwornFT','LemasSwFTPerPop','LemasSwFTFieldOps',\n",
    "           'LemasSwFTFieldPerPop','LemasTotalReq','LemasTotReqPerPop','PolicReqPerOffic','PolicPerPop',\n",
    "           'RacialMatchCommPol','PctPolicWhite','PctPolicBlack','PctPolicHisp','PctPolicAsian','PctPolicMinor',\n",
    "           'OfficAssgnDrugUnits','NumKindsDrugsSeiz','PolicAveOTWorked','LandArea','PopDens','PctUsePubTrans',\n",
    "           'PolicCars','PolicOperBudg','LemasPctPolicOnPatr','LemasGangUnitDeploy','LemasPctOfficDrugUn',\n",
    "           'PolicBudgPerPop','murders','murdPerPop','rapes','rapesPerPop','robberies','robbbPerPop','assaults',\n",
    "           'assaultPerPop','burglaries','burglPerPop','larcenies','larcPerPop','autoTheft','autoTheftPerPop','arsons',\n",
    "           'arsonsPerPop','ViolentCrimesPerPop','nonViolPerPop']\n",
    "\n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the relevant columns to use in the model \n",
    "cols_final = ['population',\n",
    " 'racepctblack',\n",
    " 'agePct12t29',\n",
    " 'numbUrban',\n",
    " 'medIncome',\n",
    " 'pctWWage',\n",
    " 'pctWInvInc',\n",
    " 'medFamInc',\n",
    " 'perCapInc',\n",
    " 'whitePerCap',\n",
    " 'PctEmploy',\n",
    " 'MalePctDivorce',\n",
    " 'MalePctNevMarr',\n",
    " 'TotalPctDiv',\n",
    " 'PctKids2Par',\n",
    " 'PctWorkMom',\n",
    " 'NumImmig',\n",
    " 'PctRecImmig8',\n",
    " 'PctRecImmig10',\n",
    " 'PctLargHouseOccup',\n",
    " 'PersPerOccupHous',\n",
    " 'PersPerRentOccHous',\n",
    " 'PctPersOwnOccup',\n",
    " 'PctPersDenseHous',\n",
    " 'HousVacant',\n",
    " 'PctHousOwnOcc',\n",
    " 'OwnOccLowQuart',\n",
    " 'OwnOccMedVal',\n",
    " 'RentLowQ',\n",
    " 'RentMedian',\n",
    " 'MedRent',\n",
    " 'MedOwnCostPctIncNoMtg',\n",
    " 'NumStreet',\n",
    " 'ViolentCrimesPerPop']\n",
    "    \n",
    "# drop all columns that are not in required list\n",
    "data.drop(data.columns.difference(cols_final), 1, inplace=True) \n",
    "\n",
    "# look at data again\n",
    "data.describe()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look at the outcome variable i.e. crime\n",
    "print(data['ViolentCrimesPerPop'].value_counts())\n",
    "\n",
    "\n",
    "# need to transform this outcome into 0 and 1's, 0 for low crime, 1 for high crime.\n",
    "# choose a suitable threshold based, < 2500 crimes is low crime, although this is subjective.\n",
    "data['ViolentCrimesPerPop'] = [0 if x < 795 else 1 for x in data['ViolentCrimesPerPop']]\n",
    "\n",
    "\n",
    "# Then need to split up the features and outcomes\n",
    "# So x as a data frame of features and y as a series of the outcome variable\n",
    "x = data.drop('ViolentCrimesPerPop', 1) \n",
    "y = data.ViolentCrimesPerPop\n",
    "\n",
    "\n",
    "print('variables', x.head(5))\n",
    "print('crime outcome', y.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at data again to see all variables and then y, the outcome, as 0s and 1s \n",
    "print(data.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use get_dummies in pandas to change categorical data to numerical\n",
    "\n",
    "print(pd.get_dummies(x['communityname']).head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning\n",
    "\n",
    "Decide which categorical variables to use in the model\n",
    "Models can only handle numerical features, so dummy features are created to transform a categorical feature into a set of dummy features, each representing a unique category. In the set of dummy features, 1 indicates that the observation belongs in that category e.g. female would be 1, male 0.\n",
    "\n",
    "Dummy features dont have to be used for ones with low frequencies, instead, buckets can be used to bucket low frequency categories as 'other'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check how many unique categories there are \n",
    "for col_name in x.columns:\n",
    "    if x[col_name].dtypes == 'object':\n",
    "        unique_cat = len(x[col_name].unique())\n",
    "        print(\"Feature '{col_name}' has {unique_cat} unique categories\".format(\n",
    "            col_name=col_name, unique_cat=unique_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the features which are not numeric\n",
    "todummy_list = ['state', 'communityname']\n",
    "\n",
    "# Dummy all categorical variables used. make them numeric and then missing values can be dealt with.\n",
    "def dummy_df(data, todummy_list):\n",
    "    for x in todummy_list:\n",
    "        dummies = pd.get_dummies(data[x], prefix=x, dummy_na=False)\n",
    "        data = data.drop(x, 1) # dropping the original feature\n",
    "        data = pd.concat([data, dummies], axis=1) # adding the one to be used\n",
    "    return data\n",
    "\n",
    "x = dummy_df(x, todummy_list)\n",
    "print(x.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Removing items with missing data\n",
    "\n",
    "Models can't handle missing data, so features with missing data should be removed. Removing data can cause issues if the data is randomly missing because it can cause the loss of a lot of data. However, greater issues arise from removing data if the data is randomly as well as non-randomly missing because this makes it no longer representative of the whole population and can introduce potential biases.\n",
    "\n",
    "Imputation can be used to replace missing values with another value i.e. the mean, median or highest frequency of a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove missing values\n",
    "# First establish how much data is missing\n",
    "x.isnull().sum().sort_values(ascending=False).head()\n",
    "\n",
    "\n",
    "# Impute the missing values using SimpleImputer in sklearn.impute\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(x)\n",
    "x = pd.DataFrame(data=imp.transform(x), columns=x.columns)\n",
    "\n",
    "\n",
    "# Check if there is still missing data\n",
    "x.isnull().sum().sort_values(ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outlier Detection\n",
    "\n",
    "An outlier = an observation that deviates drastically from other values in the dataset. Decision trees are robust to outliers because they isolate them in small regions of the feature space. Since the prediction for each leaf is the average (for regression), being isolated in seperate leaves, outliers won't influence the rest of the predictions/ impact the mean of the other leaves.\n",
    "\n",
    "**Natural vs error:\n",
    "\n",
    "Naturally occuring error, although not problematic, can skew the model by affecting the slope\n",
    "Error is indicative of data quality issues, therefore it it not information that should be used in the model. Imputation can be used to deal with these erroneous values (the same way as dealing with missing data).\n",
    "Methods of outlier detection include Kernel density estimation or Tukey IQR.\n",
    "\n",
    "**Tukey IQR** Identifies extreme values in the data and is favourable to using standard deviation from the mean to detect outliers because Tukey doesn't make assumptions about normality and is less sensitive to extreme values.\n",
    "(Interquartile ranges). To find most most extreme values, use a diff multiplier to 1.5.\n",
    "\n",
    "Q1-1.5(Q3-Q1) or above Q3+1.5(Q3-Q1)\n",
    "\n",
    "One limitation of Tukey IQR outlier detection is that it does not capture outliers in a bimodal distribution, but rather extreme values, like Kernal Density Estimation can."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find outliers using Tukey IQR\n",
    "def find_outliers_tukey(x):\n",
    "    q1 = np.percentile(x, 25)\n",
    "    q3 = np.percentile(x, 75)\n",
    "    iqr = q3-q1 \n",
    "    floor = q1 - 1.5*iqr # The floor = less than the first quartile minus the IQR.\n",
    "    ceiling = q3 + 1.5*iqr # The ceiling = more than the third quartile plus the IQR.\n",
    "    outlier_indices = list(x.index[(x < floor) | (x > ceiling)]) # If the value is below the floor, or above the ceiling, it is an outlier\n",
    "    outlier_values = list(x[outlier_indices]) # indices to access these data points later\n",
    "\n",
    "    return outlier_indices, outlier_values\n",
    "\n",
    "\n",
    "# for example, check the outliers for ‘medIncome’\n",
    "tukey_indices, tukey_values = find_outliers_tukey(x['medIncome'])\n",
    "print(np.sort(tukey_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Distribution of Features\n",
    "\n",
    "Plotting frequency histograms to show the distribution of a given feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-1-826f1a1c3cd3>, line 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-826f1a1c3cd3>\"\u001b[0;36m, line \u001b[0;32m28\u001b[0m\n\u001b[0;31m    'PctRecImmig8',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# plot histograms using peplos in marplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def histo_plot(x):\n",
    "    plt.hist(x,color='red', alpha=0.5)\n",
    "    plt.title(\"'{var_name}' Histogram\".format(var_name=x.name))\n",
    "    plt.ylabel(\"Freq\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.show()\n",
    "    \n",
    "# plot histograms to show the distributions of given features\n",
    "histo_plot(x['population'])\n",
    "histo_plot(x['racepctblack'])\n",
    "histo_plot(x['numbUrban'])\n",
    "histo_plot(x['medIncome'])\n",
    "histo_plot(x['pctWWage'])\n",
    "histo_plot(x['pctWInvInc'])\n",
    "histo_plot(x['medFamInc'])\n",
    "histo_plot(x['whitePerCap'])\n",
    "histo_plot(x['PctEmploy'])\n",
    "histo_plot(x['MalePctDivorce'])\n",
    "histo_plot(x['MalePctNevMarr'])\n",
    "histo_plot(x['TotalPctDiv'])\n",
    "histo_plot(x['PctKids2Par'])\n",
    "histo_plot(x['PctWorkMom'])\n",
    "histo_plot(x['NumImmig'])\n",
    "histo_plot(x['PctRecImmig8'])\n",
    "histo_plot(x['PctRecImmig10'])\n",
    "histo_plot(x['PctLargHouseOccup'])\n",
    "histo_plot(x['PersPerRentOccHous'])\n",
    "histo_plot(x['PctPersOwnOccup'])\n",
    "histo_plot(x['PctPersDenseHous'])\n",
    "histo_plot(x['HousVacant'])\n",
    "histo_plot(x['PctHousOwnOcc'])\n",
    "histo_plot(x['OwnOccLowQuart'])\n",
    "histo_plot(x['OwnOccMedVal'])\n",
    "histo_plot(x['RentLowQ'])\n",
    "histo_plot(x['RentMedian'])\n",
    "histo_plot(x['MedRent'])\n",
    "histo_plot(x['MedOwnCostPctIncNoMtg'])\n",
    "histo_plot(x['NumStreet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histograms showing the distribution of features by the outcome variable (dependent variable)\n",
    "def histogram_dv(x,y):\n",
    "    plt.hist(list(x[y==0]), alpha=0.5, label='DV=0')\n",
    "    plt.hist(list(x[y==1]), alpha=0.5, label='DV=1')\n",
    "    plt.title(\"'{var_name}' Histogram by DV Category\".format(var_name=x.name))\n",
    "    plt.ylabel(\"Freq\")\n",
    "    plt.xlabel(\"Value\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "    \n",
    "# these show the distribution of a feature when the outcome is 0, so crime is less than .../none\n",
    "histo_plot_dv(x['population'], y)\n",
    "histo_plot_dv(x['racepctblack'], y)\n",
    "histo_plot_dv(x['numbUrban'], y)\n",
    "histo_plot_dv(x['medIncome'], y)\n",
    "histo_plot_dv(x['pctWWage'], y)\n",
    "histo_plot_dv(x['pctWInvInc'], y)\n",
    "histo_plot_dv(x['medFamInc'], y)\n",
    "histo_plot_dv(x['whitePerCap'], y)\n",
    "histo_plot_dv(x['PctEmploy'], y)\n",
    "histo_plot_dv(x['MalePctDivorce'], y)\n",
    "histo_plot_dv(x['MalePctNevMarr'], y)\n",
    "histo_plot_dv(x['TotalPctDiv'], y)\n",
    "histo_plot_dv(x['PctKids2Par'], y)\n",
    "histo_plot_dv(x['PctWorkMom'], y)\n",
    "histo_plot_dv(x['NumImmig'], y)\n",
    "histo_plot_dv(x['PctRecImmig8'], y)\n",
    "histo_plot_dv(x['PctRecImmig10'], y)\n",
    "histo_plot_dv(x['PctLargHouseOccup'], y)\n",
    "histo_plot_dv(x['PersPerRentOccHous'], y)\n",
    "histo_plot_dv(x['PctPersOwnOccup'], y)\n",
    "histo_plot_dv(x['PctPersDenseHous'], y)\n",
    "histo_plot_dv(x['HousVacant'], y)\n",
    "histo_plot_dv(x['PctHousOwnOcc'], y)\n",
    "histo_plot_dv(x['OwnOccLowQuart'], y)\n",
    "histo_plot_dv(x['OwnOccMedVal'], y)\n",
    "histo_plot_dv(x['RentLowQ'], y)\n",
    "histo_plot_dv(x['RentMedian'], y)\n",
    "histo_plot_dv(x['MedRent'], y)\n",
    "histo_plot_dv(x['MedOwnCostPctIncNoMtg'], y)\n",
    "histo_plot_dv(x['NumStreet'], y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering- \n",
    "Either 1. increase the dimensionality or 2. decrease the dimentionality\n",
    "\n",
    "Increasing Dimensionality = creating new features. This is useful if the impact of two or more features on the outcome is non-additive. A good automated way to do this is to look for interactions between features.\n",
    "e.g. a simple 2-way interaction (where X3 is the interaction between X1 and X2):\n",
    "\n",
    "X3-X1 * X2\n",
    "\n",
    "However, with lots of features, this grows the data A LOT. Therefore, it is better t use domain knowledge about certain interactions between features so that there aren't too many interaction terms.\n",
    "\n",
    "Dimensionality has benefits because information is added, however, it is computationally costly i.e. inefficient and also has the potential for overfitting the model. So it is a trade off between creating new useful information vs the potential for overfitting plus the computational cost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# polynomialFeatures in sklearn.preprocessing to create two-way interactions for ALL features\n",
    "# not implemented because it would make the program too computationally slow \n",
    "# from itertools import combinations\n",
    "# from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# def add_interactions(df):\n",
    "    # get feature names\n",
    "    #combos = list(combinations(list(df.columns),2))\n",
    "   # colnames = list(df.columns) + ['_'.join(x) for x in combos]\n",
    "\n",
    "    # establish the interactions in the data\n",
    "   # poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "   # df = poly.fit_transform(df)\n",
    "   # df = pd.DataFrame(df)\n",
    "   # df.columns = colnames\n",
    "\n",
    "    # remove the interactions with 0 values\n",
    "    #noint_indicies = [i for i, x in enumerate(list((df == 0).all())) if x]\n",
    "   # df = df.drop(df.columns[noint_indicies], axis=1)\n",
    "\n",
    "   # return df\n",
    "\n",
    "\n",
    "#x = add_interactions(x)\n",
    "#print(x.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decreasing dimensionality**\n",
    "\n",
    "Using principal component analysis(PCA), a method that transforms a dataset with many features into its principle components that best summarise the underlying variance in the data.\n",
    "\n",
    "Each ‘principle component’ is established by finding the linear combination of features that maximises variance, whilst also ensuring zero correlation with previously calculated principal components.\n",
    "\n",
    "PCA/ decreasing dimensionality is useful when you have very high-dimensionality data, in order to reduce dimensions, when the dataset has many highly correlated variables because it will take the variance from these to reduce this correlation and when there is poor observation-to-feature ratio.\n",
    "\n",
    "However, using dimensionality reduction makes the data harder to interpret and understand because the output gives arbitrary principle components, e.g. for interpreting the outcome, principle component number 1 is not as easy to interpret as medIncome. Therefore, in certain contexts, like explaining the results to a client, this would make it difficult to explain the drivers of the target outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not implementing this either, because it makes the results too difficult to interpret\n",
    "# Using sklearn.decomposition PCA to find the principal components \n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# transform entire data set into 10 features\n",
    "# pca = PCA(n_components=10)\n",
    "# x_pca = pd.DataFrame(pca.fit_transform(x))\n",
    "\n",
    "# print(x_pca.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**End of pre-processing"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scikit-ML",
   "language": "python",
   "name": "scikit-ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
